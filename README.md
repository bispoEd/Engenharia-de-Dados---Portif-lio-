ğŸ§­ Estrutura por NÃ­vel
ğŸ”° NÃ­vel Iniciante

    Projeto 01 â€“ Coleta de dados com Python (API OpenWeather)
    ExtraÃ§Ã£o de dados em tempo real via API, tratamento simples e exportaÃ§Ã£o para CSV.

    Projeto 02 â€“ ETL com Pandas (dados de vendas)
    Pipeline de extraÃ§Ã£o, limpeza e transformaÃ§Ã£o de dados de vendas simulados.

    Projeto 03 â€“ Dashboard com Streamlit
    VisualizaÃ§Ã£o interativa dos dados de vendas com grÃ¡ficos e filtros.

ğŸ” NÃ­vel IntermediÃ¡rio

    Projeto 04 â€“ ETL automatizado com Apache Airflow (local)
    CriaÃ§Ã£o de DAGs para automatizar a rotina de processamento de dados com agendamento diÃ¡rio.

    Projeto 05 â€“ Armazenamento em Banco de Dados (PostgreSQL)
    InserÃ§Ã£o dos dados processados em uma tabela relacional utilizando SQLAlchemy.

    Projeto 06 â€“ IntegraÃ§Ã£o com Data Warehouse (BigQuery)
    Upload dos dados para o Google BigQuery com autenticaÃ§Ã£o via Service Account, prontos para anÃ¡lises em larga escala.

ğŸš€ Principais Habilidades Demonstradas

    Coleta de dados via API e arquivos locais

    Processamento e transformaÃ§Ã£o com Pandas

    VisualizaÃ§Ã£o com Streamlit + Altair

    OrquestraÃ§Ã£o de tarefas com Apache Airflow

    PersistÃªncia com PostgreSQL

    IntegraÃ§Ã£o com Google BigQuery

    OrganizaÃ§Ã£o modular e reutilizÃ¡vel de pipelines


ğŸ“Œ PrÃ³ximos passos (nÃ­vel avanÃ§ado)

    Pipeline distribuÃ­do com Databricks + PySpark

    Monitoramento e data observability

    Qualidade de dados com ferramentas como Great Expectations

    Projeto Capstone completo de ponta a ponta com arquitetura em nuvem

Esse programa foi construÃ­do de forma didÃ¡tica e escalÃ¡vel, sendo ideal para comprovar minhas habilidades tÃ©cnicas e evoluÃ§Ã£o prÃ¡tica na Ã¡rea de Engenharia de Dados.
